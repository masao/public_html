\documentstyle[a4j,eclepsf,fancyheadings]{jarticle}

\thispagestyle{fancy}
\lhead{1999年2月16日}
\chead{情報認識論III（パターン認識）}
\rhead{98513 高久雅生}
\begin{document}

\begin{center}
\bf\fbox{\fbox{舟久保登. パターン認識. 東京, 共立出版, 1991, 172p.}}
\end{center}

\setcounter{section}{2}

 \section{統計的決定によるパターン認識}

本章ではパターンの発生する確率がわかっている場合のパターン認識を取り扱う。
前章で個々の入力パターンの照合による識別では、
入力パターンがどのような確率で発生するかは考慮していなかった。
しかし、もし大量の入力パターンを処理する際には、
発生の確率を考慮することは識別の向上に影響する。

このようなパターン識別は一般的な統計的決定理論の枠組を利用する。
以下本章では、
入力パターンの発生確率が判明している場合、
またその形が正規分布を示している場合、
さらに正規分布の際の平均値や分散の推定による学習法について、
簡単に紹介する。

  \subsection{ベイズ識別法}

最小誤り率をもたらす識別法をベイズ識別法という。

\subsubsection*{A. 事前確率のみがわかっている場合}

{\bf[例]}\quad
入力パターンは英文字パターンで、その識別を行なう。
各英字の事前生起確率は次の表の通り。

\begin{center}
 { 表3.1 英字に対する事前生起確率}\\
\begin{tabular}[tb]{c|r@{~}||c|r@{~}||c|r@{~}}
 \hline
 \small 英字& \small 確率(\%)& \small 英字& \small 確率(\%)& \small 英字& \small 確率(\%)\\ \hline
 A& 8.29&  J& 0.21& S& 6.33 \\
 B& 1.43&  K& 0.48& T& 9.27\\
 C& 3.68&  L& 3.68& U& 2.53\\
 D& 4.29&  M& 3.23& V& 1.03\\
 E& 12.08& N& 7.16& W& 1.62\\
 F& 2.20&  O& 7.28& X& 0.20\\
 G& 1.71&  P& 2.93& Y& 1.57\\
 H& 4.54&  Q& 0.11& Z& 0.09\\
 I& 7.16&  R& 6.90& &
 \\ \hline
\end{tabular}
\end{center}

もし入力パターンを見ずに識別する場合は、
英字Eが最大の確率を示すことから、つねに
入力パターンのカテゴリ名は``E''である。
とすると誤り率をもっとも少なくする。

以上の例は次のように一般化できる。

今カテゴリを$\omega_l, (l = 1, 2, \cdots , L)$とし、
その事前生起確率を$P(\omega_l)$とする。
この時、ある入力パターン{\boldmath{$x$}}のカテゴリを
$\omega_l$に識別した時の誤り率は
\[
 誤り率\colon\qquad 1-P(\omega_l)
\]
となる。

\subsubsection*{B. 事後確率がわかっている場合}

{\bf[例]}\quad
前項の英字パターンの識別について、事後確率がわかった場合。

入力パターンの観測結果から左上に弧状の線分を持ったパターンであることが判明し、
これは英字では``C'', ``G'', ``O'', ``Q''の場合のみ現れる。

そこで、ベイズの定理による式を元に事後確率が求まる。
たとえば、 ``C''について、
\begin{eqnarray*}
\displaystyle  P(\omega_l|\mbox{\boldmath$x$})_{\omega_{l}=C} = &
  \frac{\displaystyle 3.68\cdot 1}{\displaystyle 3.68\cdot 1 + 1.71\cdot 1 + 7.28\cdot 1 + 0.11 \cdot 1}\\
 & = \displaystyle \frac{3.68}{12.78} = 0.2879
\end{eqnarray*}
と算出できる。
同様に各カテゴリの事後確率を以下に示す。
\begin{center}
{\small 表3.2\quad 観測後の各カテゴリに対する事後生起確率値}\\
 \begin{tabular}{c|r@{~}}
 \hline
 \small 英字& \small 確率(\%)  \\ \hline
 C& 28.79\\
 G& 13.38\\
 O& 56.97\\
 Q& 0.86\\
 その他& 0\\
 \hline
 \end{tabular}
\end{center}

以上から事後生起率は ``O''で最大値を持つことがわかる。
したがって、誤り率を最小とする識別結果は ``O''となる。
A.での ``E''とは異なっており、
誤り率は87.92\%から43.03\%に改善される。

\subsubsection*{C. 正規分布型条件付確率}

統計的決定によるパターン識別を行なう際の基本的な流れを前項で述べた。
ここでは、この場合の識別関数の特性について述べる。

特に、
条件付確率が統計的に扱いやすい
正規分布確率を持っている場合を取り上げる。

%まずカテゴリ条件付確率は次のように書ける。
%
%\[
%P(\mbox{\boldmath{$x$}}|\omega_l)=
%\frac{1}{(2\pi)^{\frac12}|\mit\Sigma^{(l)}|^{\frac12}}\cdot
%e^{-\frac12(\mbox{\boldmath{$x$}}-\mbox{\boldmath{$\mu$}}^{(l)})^T\mit\Sigma^{(l)-1}(\mbox{\boldmath{$x$}}-\mbox{\boldmath{$\mu$}}^{(l)})}
%\]
%
%一方、識別関数$g^{(l)}(\mbox{\boldmath$x$})$は一般にベイズの定理の形であり、
%
%
%上のようになる。
%
%これを展開し、最終的な識別関数の形は次のようになる。
%
%！！！式(3.5)！！！
%
%これにより、識別は、
%
%もし！！！ならば、{\boldmath{$x$}}のカテゴリ名は$l_0$である。

この場合の識別関数の性質は、

\begin{itemize}
 \item 一般的には、\\
       非線形な2次識別関数となる。
 \item $\mit\Sigma^{(l)}=\mit\Sigma$（カテゴリに独立）ならば、\\
       線形（1次）の識別関数となる。
 \item $\mit\Sigma^{(l)}=\sigma^2I$（要素が一定の対角行列）の場合、\\
       識別関数は線形識別関数となる。
\end{itemize}

従って、
正規分布確率に対するベイズ識別法は、
線形と2次の識別関数をもたらすことになる。

\subsubsection*{D. 条件付危険量による定式化}

これまでのベイズ識別法の説明では誤り率最小という条件を使用してきたが、
厳密には損失量という概念を使い、
条件付危険量の最小化を要求する。

損失量は本来カテゴリ$\omega_l$に属するパターン{\boldmath{$x$}}を、
誤って別のカテゴリ$\omega_\nu$に属すると決めてしまった時に生じる量である。
一般にある{\boldmath{$x$}}を$\omega_\nu$であると決めた時の条件付危険量$R(\omega_\nu|\mbox{\boldmath{$x$}})$は式(3.6)で与えられる。
本来のベイズ識別法は、これを最小とするカテゴリ名を出力する。
すなわち、

\[
 R(\omega_\nu|\mbox{\boldmath{$x$}})=\min_{1\leq l'\leq L}R(\omega_\nu|\mbox{\boldmath{$x$}})\mbox{ならば、{\boldmath{$x$}}のカテゴリ名は$l_0$である。}
\]

この時、決定が正しければ損失0、誤りならば1とする単純なモデルの場合には、
前項まで使ってきたベイズ識別法とおなじものになる。

  \subsection{最尤法}

ベイズ識別法において、
事前生起確率$P(\omega_l)$が全てのカテゴリに対して等しい時のことを考えてみる。
すると最小誤り率の達成は
カテゴリ条件付確率$P(\mbox{\boldmath$x$}|\omega_l)$の最大化になる。

この$P(\mbox{\boldmath$x$}|\omega_l)$はカテゴリ$\omega_l$における{\boldmath$x$}の尤度(likelihood)と呼ばれ、この識別法は最尤法と呼ばれている。


  \subsection{統計的学習}

  \subsubsection*{A. 統計的学習の分類}

ここまで述べてきたベイズ識別法や最尤法を適用するには、
確率分布の具体的な形や、平均や分散などのパラメータ値が必要であった。

しかし、時によってはそのようなデータが存在せず、
対象パターンの一部を利用して
分布の形やパラメータ値を求めなければならないこともある。
このための処理を一種の学習と呼び、
学習のために使うパターンを学習パターンと呼ぶ。

統計学習には、
2つの側面からの分類ができる。

\begin{enumerate}
 \item パラメトリック・ノンパラメトリック

       パターンである確率変数が
       正規分布のような確定した分布系をなせば、
       パラメトリック、そうでなければノンパラメトリックとなる。
       分布形状を決定するパラメータ値
       （正規分布では平均や分散）を決めれば良いのでこの名がついた。

 \item 教師つき・教師なし

       利用する学習パターンがあらかじめどのカテゴリに属するかが
       わかっている場合を教師付きと呼ぶ。
       逆にこのような情報がわからない時には教師なしと呼ぶ。
\end{enumerate}

学習の複雑さと難しさは使用できる情報がどれだけあるかによる。
最大の情報不足はノンパラメトリックな教師なし学習である。

以下ではこの内、
最もわかりやすいパラメトリックで教師つきの学習である、
最尤推定とベイズ推定学習法を紹介する。

  \subsubsection*{B. 最尤推定学習法}

カテゴリ条件付確率の分布形を想定し、
そのパラメータ{\boldmath{$\theta$}}$_l$を推定する場合を考える。
その際に使用する$K$個からなる個々の学習パターンを
{\boldmath{$x$}}$_k^{(l)}$とする。
すると、
\[
 \mbox{カテゴリ条件付確率}\colon\qquad P(\mbox{\boldmath{$x$}}_k^{(l)}|\ \omega_l, \mbox{\boldmath{$\theta$}}_l)\qquad (k = 1, 2, \cdots, K)
\]

となる。

分布型を正規分布と考えた場合、
推定すべきパラメータは平均値{\boldmath{$\mu$}}と分散$\mit\Sigma$になる。
各々の推定値$\hat{\mbox{\boldmath{$\mu$}}}, \hat{\mit\Sigma}$を求めると、
\begin{equation}
 \hat{\mbox{\boldmath{$\mu$}}}=\frac{1}{K}\sum_{k=1}^{K}\mbox{\boldmath{$x$}}_k, \qquad
  \hat{\mit\Sigma}=\frac1K\sum_{k=1}^{K}(\mbox{\boldmath{$x$}}_k-\hat{\mbox{\boldmath{$\mu$}}})(\mbox{\boldmath{$x$}}_k-\hat{\mbox{\boldmath{$\mu$}}})^T
\end{equation}
となる。
こうして求められたパラメータを使って、
ベイズ識別法や最尤識別法を適用できる。
%
%  \subsubsection*{C. ベイズ推定学習法}
%
%パラメータ{\boldmath{$\theta$}}を確率変数とする。
%そこで{\boldmath{$\theta$}}は学習パターン\it Xに条件付けられており
%（したがって事後確率）
%\[
% \mbox{\boldmath{$\theta$}の条件付確率}\colon\qquad P(\mbox{\boldmath{$\theta$}}|X)
%\]
%と書ける。
%
%


\end{document}
